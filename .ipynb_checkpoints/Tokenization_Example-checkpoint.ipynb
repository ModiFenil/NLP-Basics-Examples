{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "467ad944-d0db-4a3a-a183-93692ccba9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13e1ee93-a9b3-4605-8f51-2401036140d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello welcome to tokenization example tutorial.\n",
    "i am study about NLP to become expert.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaa3bc33-9f6a-47c2-900f-fc212352c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf2a30d4-d072-4c09-a76a-fbadfedeeb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "346f5c93-e938-4fec-926c-e3f39bc9ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## paragraph-->words\n",
    "## sentence-->words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0480a69e-019d-459e-8be0-773b28837048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'tokenization',\n",
       " 'example',\n",
       " 'tutorial',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'study',\n",
       " 'about',\n",
       " 'NLP',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d39c7d6-e412-415f-b5d8-793c7a9a5a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'tokenization', 'example', 'tutorial', '.']\n",
      "['i', 'am', 'study', 'about', 'NLP', 'to', 'become', 'expert', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a65be12e-f8f6-47dc-84ad-35a8b68c305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc68f30b-63e3-4d77-94d1-6f6e9e26d175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'tokenization',\n",
       " 'example',\n",
       " 'tutorial',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'study',\n",
       " 'about',\n",
       " 'NLP',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92362f35-c469-4290-b381-339fe47cf693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79a4e8b1-e5f1-4ed9-bf8d-bd9043bb040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "697897a7-d1ac-423f-b7de-54496159c484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'tokenization',\n",
       " 'example',\n",
       " 'tutorial.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'study',\n",
       " 'about',\n",
       " 'NLP',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c4bb7-b27d-4409-8949-44c59095b015",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9fa338c-bb9b-42f0-9cae-5e4fee6ccefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "words= ['eating','eats','writing','writes','programming','programs',\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2641cb-aedb-48cb-a4b2-f17bac29dafd",
   "metadata": {},
   "source": [
    "## Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5b572e3-a43a-48fe-ba65-8fdafffc5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b3230d4-9f29-4874-bead-5f2878a02a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cee90c70-e9a1-4f8d-8b24-2c3be810db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>histori\n",
      "finally------>final\n",
      "finalized------>final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "131487b5-d55b-4062-882e-c76cbf338c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congretul'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('Congretulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e9cd6c4-7196-48b2-ba78-8273b8245a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('sitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4888ad-6daf-470a-922f-951c1bb4e487",
   "metadata": {},
   "source": [
    "# Regexpstemmer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56c79038-7bf2-4162-8f5c-67a752b8a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f5d5fce-2da2-4f67-9f11-caf3542d039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d04c2be9-edc0-4900-b292-2a9cf89dd559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56b51f30-afbc-48f5-9491-b2c0b82fd8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "373f5730-5aa6-4213-ba4e-42c5e1b7bd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cap'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"capable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ea97ac2-0fe1-4308-93ab-b7e03c7991cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new'\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"new's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e7984-508a-4101-848c-0231fd9f2e5d",
   "metadata": {},
   "source": [
    "## Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e386f9b-91b9-468d-a0e5-0cd3b2335f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cd31f64-03c5-43ea-a5a2-f27ca392ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "acd7d8c6-3344-4e9c-a80c-3b12651cca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating-------->eat\n",
      "eats-------->eat\n",
      "writing-------->write\n",
      "writes-------->write\n",
      "programming-------->program\n",
      "programs-------->program\n",
      "history-------->histori\n",
      "finally-------->final\n",
      "finalized-------->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-------->\"+ snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a20e51fc-9b70-4e2a-9547-aadb44271a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0602a896-f4bc-4812-bf8d-e51afad2586b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " snowball_stemmer.stem(\"fairly\"),snowball_stemmer.stem(\"sportingly\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
